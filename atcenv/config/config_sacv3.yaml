experiment_name: "SAC_v5_additive_basic_lower_std_spread"
environment:
  class_path: atcenv.src.environment_objects.environment.DefaultEnvironment
  init_args:
    render_frequency: 0
model:
  class_path: atcenv.src.models.sac.SAC_V3
  init_args:
    action_dim: 2
    alpha_lr: 3e-4
    actor_lr: 3e-4
    critic_q_lr: 3e-3
    gamma: 0.99
    tau: 5e-3
    policy_update_freq: 8
    initial_random_steps: 0
    buffer:
      class_path: atcenv.src.models.replay_buffer.ReplayBuffer
      init_args:
        obs_dim: 8
        action_dim: 2
        n_agents: 20
        size: 20000000
        batch_size: 2048
    actor:
      class_path: atcenv.src.models.actor.MultiHeadAdditiveActorBasic
      init_args:
        q_dim: 8
        kv_dim: 5
        num_heads: 3
        out_dim: 2
        log_std_min: -3
        log_std_max: 1
    critic_q:
      class_path: atcenv.src.models.critic_q.MultiHeadAdditiveCriticQv3Basic
      init_args:
        q_dim: 10
        kv_dim: 5
        num_heads: 3
    critic_q_target:
      class_path: atcenv.src.models.critic_q.MultiHeadAdditiveCriticQv3Basic
      init_args:
        q_dim: 10
        kv_dim: 5
        num_heads: 3
scenario:
  num_episodes: 10000
  num_flights: 20
  airspace_area: 10000000000
  traffic_density: 0
  test_scenario_dir: "hello"
  num_test_episodes: 0
  test_frequency: 0
  random_seed: 1
airspace:
  class_path: atcenv.src.environment_objects.airspace.EnrouteAirspace
aircraft:
  min_speed: 200
  max_speed: 250
  min_distance: 10000
observation:
  class_path: atcenv.src.observation.observation.GlobalTrack
  init_args:
    observation_size: 8
    normalize_data: True
    create_normalization_data: False
    normalization_data_file: "global_normalization.p"
    normalization_samples: 50000
reward:
  class_path: atcenv.src.reward.reward.DefaultReward
  init_args:
    intrusion_weight: -1
    drift_weight: -0.1
logger:
  class_path: atcenv.src.logger.logger.RLLoggerV3
  init_args:
    log_frequency: 25
    verbose: True
